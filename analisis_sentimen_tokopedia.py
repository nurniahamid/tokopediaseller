# -*- coding: utf-8 -*-
"""Analisis_Sentimen_Tokopedia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uypSqYEfu-N35inS9cRUIgZKLSz0rkaW
"""

!pip install numpy==1.24.4 scipy==1.11.4 gensim==4.3.1

"""# **Pengenalan**

- Nama : Nurnia Hamid
- Email : a309xaf385@devacademy.id
- Submission 1 : Belajar Pengemmbangan Machine Learning

- Proyek : Analisis Sentimen Review Aplikasi Tokopedia Seller

- Sumber data : Scraping review di google play store aplikasi 'Tokepedia Seller'

- link : https://play.google.com/store/search?q=tokopedia%20seller&c=apps&hl=id

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.svm import SVC
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import CountVectorizer

"""# **Load Data**

Data telah di scraping dan diupload ke GitHub
"""

url = "https://raw.githubusercontent.com/nurniahamid/tokopediaseller/refs/heads/main/review_tokopedia_seller.csv"
df = pd.read_csv(url)
df.head()

"""Kita menghilangkan beberapa kolom dan hanya menyisakan kolom yang akan digunakan dalam analisis sentimen yaitu 'userName', 'content', 'score'"""

df = df[['userName', 'content', 'score']]
df.head()

"""# **EDA & Text_Preprocessing**"""

df.info()

"""Dataset yang akan kita gunakan berjumlah 11000"""

df.isnull().sum()

"""dari hasil yang didapatkan, tidak terdapat lagi nilai null dalam data

Selanjutnya kita masuk ketext Pre-Processing data

- Case Folding bersungsi untuk mengubah huruf besar menjadi huruf kecil dengan menggunakan fungsi lower()
- case folding berfungsi untuk membersihkan data, dalam proeyek ini kita melakukan cleansing dengan menghapus url, menghapus mention, tanda baca, angka, karakter newline, karakter non-ASCII dan menghapus spasi awal dan spasi akhir yang ada di teks
"""

# case folding

def case_folding(content):
  if isinstance(content,str):
    content = content.lower()
  return content
df['case_folding'] = df['content'].apply(case_folding)

# cleansing
import string
def cleansing(content):
    if isinstance(content, str):
        content = re.sub(r"http\S+", "", content)
        content = re.sub(r"@[A-Za-z0-9_]+", "", content)
        content = content.translate(str.maketrans("", "", string.punctuation))
        content = re.sub(r'\d+', '', content)
        content = content.replace('\n', ' ')
        content = re.sub(r'[^\x00-\x7f]', r'', content)
        content = content.strip(' ')
    return content
df['cleansing'] = df['case_folding'].apply(cleansing)
df.head()

"""Selanjutnya proses tokenisasi
- tokenisasi berfungsi memecah teks atau kalimat menjadi kata per kata atau yang disebut token, Pada proyek ini kita menggunakan nltk dalam proses tokenisasi
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#tokenisasi
nltk.download('punkt_tab')
df['tokenized'] = df['cleansing'].apply(word_tokenize)
df.head()

"""selanjutnya tahapan normalisasi
- normaliasi berfungsi untuk mengubah kata-kata tidak baku menjadi bentuk baku/standard, agar analisis teks kita di proyek ini menjadi lebih akurat dan konsisten
"""

#normalisasi
kamus_normalisasi = {
    "gak": "tidak",
    "ga": "tidak",
    "nggak": "tidak",
    "gk": "tidak",
    "yg": "yang",
    "aja": "saja",
    "udah": "sudah",
    "tokped": "tokopedia",
    "bgt": "banget",
    "trs": "terus",
    "tp": "tapi",
    "dr": "dari",
    "dgn": "dengan",
    "klo": "kalau",
    "blm": "belum",
    "sdh": "sudah",
    "sm": "sama",
    "nya": "",
    "nyaa": "",
    "bikin": "membuat",
    "aja": "saja",
    "kalo": "kalau",
    "tdk": "tidak",
    "bgtu": "begitu",
    "bantuam": "bantuan",
    "jd": "jadi",
    "mantab": "mantap",
    "banget": "sangat",
}
def normalisasi_kata(tokens, kamus):
    return [kamus.get(word.lower(), word.lower()) for word in tokens]
df['normalized'] = df['tokenized'].apply(lambda tokens: normalisasi_kata(tokens, kamus_normalisasi))

"""selanjutnya stopwords
- stopword ini adalah kata-kata umum yang sering muncul dalam teks, tetapi bisanya tidak membawa makna penting untuk proses analisis, sama seperti tokenisasi kita juga menggunakan nltk yaitu modul stopwords
"""

from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('indonesian'))

df['stopwords'] = df['normalized'].apply(lambda tokens: [word for word in tokens if word not in stop_words and word.isalpha()])

"""menggabungkan kembali daftar kata (token) yang sebelumnya telah difilter dari stopword, menjadi teks utuh dalam bentuk string"""

df['stopwords_text'] = df['stopwords'].apply(lambda tokens: ' '.join(tokens))

"""Instal Sastrawi untuk diguanakn dalam proses stemming"""

!pip install Sastrawi

"""- Stemming ini berfungsi mengubah kata ke bentuk dasarnya, untuk membantu model fokus pada makna inti kata, disini kita menggunakan library Sastrawi untuk stemming bahasa indonesia"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from tqdm import tqdm
tqdm.pandas()

factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stemming_per_token(token_list):
    return [stemmer.stem(token) for token in token_list]

df['stemming'] = df['stopwords_text'].progress_apply(lambda text: stemmer.stem(text))

df.head()

"""# **Pelabelan**

Pelabelan ini adalah tahapan untuk kita memberikan target/label kelas pada teks, agar bisa diklasifikasi sentimen (positif, negatif, dan netral)

Karena kita melakukan pelabelan menggunakan Text Blob, yang mana hanya bisa untuk data berbahasa inggris, jadi kita perlu menterjemahkan data terlebih dahulu kedalam bahasa inggris. Disini kita menggunakan deep translator
"""

!pip install deep-translator

from deep_translator import GoogleTranslator
from tqdm import tqdm

def translate_text(tokens):
    try:
        if isinstance(tokens, list):
            text = ' '.join(tokens)
        else:
            text = tokens
        return GoogleTranslator(source='auto', target='en').translate(text)
    except Exception as e:
        return f"Error: {e}"

tqdm.pandas()
df['translated'] = df['stemming'].progress_apply(translate_text)
df[['stemming', 'translated']].head()

"""setelah ditranslate, selanjutnya kita lakukan pelabelan pada teks menggunakan text blob"""

from textblob import TextBlob

def get_polarity(text):
    try:
        return TextBlob(text).sentiment.polarity
    except:
        return None

def get_subjectivity(text):
    try:
        return TextBlob(text).sentiment.subjectivity
    except:
        return None

def get_sentiment_label(polarity):
    try:
        if polarity > 0:
            return 'positif'
        elif polarity < 0:
            return 'negatif'
        else:
            return 'netral'
    except:
        return 'error'

df['polarity'] = df['translated'].apply(get_polarity)
df['subjektivitas'] = df['translated'].apply(get_subjectivity)
df['sentimen'] = df['polarity'].apply(get_sentiment_label)
df[['translated', 'polarity', 'subjektivitas', 'sentimen']].head()

sentiment_counts = df['sentimen'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('viridis'))
plt.title('Proporsi Sentimen')
plt.axis('equal')
plt.show()

sentiment_counts = df['sentimen'].value_counts()
print(sentiment_counts)

"""Dari hasil yang didapatkan jumlahnya sentimen netral paling banyak ditemukan, setelahnya sentimen positif, dan sentimen negatif hanya sedikit atau hanya 11% dari data

"""

df.head()

"""**Berikut gambar WordCloud dari masing-masing sentimen**"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

text_positif = ' '.join(df[df['sentimen'] == 'positif']['stemming'].dropna())
wordcloud_positif = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_positif)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positif, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud untuk Sentimen: Positif', fontsize=16)
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

text_negatif = ' '.join(df[df['sentimen'] == 'negatif']['stemming'].dropna())
wordcloud_negatif = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_negatif)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negatif, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud untuk Sentimen: Negatif', fontsize=16)
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

text_netral = ' '.join(df[df['sentimen'] == 'netral']['stemming'].dropna())
wordcloud_netral = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text_netral)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_netral, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud untuk Sentimen: Netral', fontsize=16)
plt.show()

"""# **Model**

### **SKEMA 1**

RANDOM FOREST + TF_IDF (untuk rf)

tf_idf untuk rf yaitu sebagai input model dimana dalam tahapan ini kita mengubah dokumen teks menjadi matriks angka/numerik, tf_idf untuk rf ini menggunakan unigram, kemudian kata yang muncul di >85% akan diabaikan dan max_features yang diambil 1500
"""

tfidf_rf = TfidfVectorizer(
    ngram_range=(1,1),
    max_df=0.85,
    min_df=5,
    max_features=1500,
    sublinear_tf=True
)

"""kemudian kita splitting datanyan 70% untuk daat pelatihan(train) dan 30% untuk pengujian(test)"""

X_tfidf_rf = tfidf_rf.fit_transform(df['stemming'])
X_train_rf, X_test_rf, y_train, y_test = train_test_split(
    X_tfidf_rf, df['sentimen'], test_size=0.3, random_state=42
)

"""**Model Random Forest dan Evaluasi Model**"""

model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train_rf, y_train)
y_pred_rf = model_rf.predict(X_test_rf)

print("Skema 1: Random Forest + TF-IDF")
print("Akurasi:", accuracy_score(y_test, y_pred_rf))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_rf))

"""### **SKEMA 2**

SGDClassifier + TF-IDF

SGD = Stochastic Gradient Descent

pada algoritma ini kita juga menggunakan tf_idf yaitu mengubah teks menjadi representasi numerik dalam proyek ini kita juga menerapkan dalam tf_idf yaitu menangkap konteks dengan kata bigram, dan memfilter kata yang terlalu umum.
"""

tfidf_sgd = TfidfVectorizer(
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.9,
    sublinear_tf=True,
    norm='l2',
    max_features=5000
)
X_tfidf_sgd = tfidf_sgd.fit_transform(df['stemming'])

"""split data latih dan data uji"""

X_train_sgd, X_test_sgd, y_train, y_test = train_test_split(
    X_tfidf_sgd, df['sentimen'], test_size=0.3, random_state=42
)

"""**Model SGDClassifier dan Evaluasi Model**"""

model_sgd = SGDClassifier(
    loss='hinge',
    penalty='l2',
    alpha=1e-4,
    random_state=42,
    max_iter=3000,
    tol=1e-3
)
model_sgd.fit(X_train_sgd, y_train)
y_pred_sgd = model_sgd.predict(X_test_sgd)

print("\nSkema 2: SGDClassifier  + TF-IDF ")
print("Akurasi:", accuracy_score(y_test, y_pred_sgd))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_sgd))

"""### **SKEMA 3**

RANDOM FOREST + Bag of Words

pada skema 3 ini kita menggunakan CountVectorizer untuk mengubah teks menjadi frekuensi kemunculan kata

kita juga menggunakan unigram dan bigram (kata tunggal & dua kata berurutan)

kita mengatur max_feature 5000 kata atau frasa berdasarkan frekuensi teratas
"""

vectorizer_bow = CountVectorizer(ngram_range=(1, 2), max_features=5000)
X_bow = vectorizer_bow.fit_transform(df['stemming'])

"""split data latih dan data uji"""

X_train_bow, X_test_bow, y_train, y_test = train_test_split(
    X_bow, df['sentimen'], test_size=0.3, random_state=42
)

"""**Model Random Forest dan Evaluasi Model**"""

model_rf_bow = RandomForestClassifier(n_estimators=200, random_state=42)
model_rf_bow.fit(X_train_bow, y_train)
y_pred_bow = model_rf_bow.predict(X_test_bow)


print("\nSkema 3: Random Forest + Bag of Words")
print("Akurasi:", accuracy_score(y_test, y_pred_bow))
print("Laporan Klasifikasi:\n", classification_report(y_test, y_pred_bow))

